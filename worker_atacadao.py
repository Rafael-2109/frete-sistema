#!/usr/bin/env python3
"""
Worker dedicado para processar agendamentos do AtacadÃ£o
Executa jobs assÃ­ncronos via Redis Queue

Uso:
    python worker_atacadao.py
    
    # Ou para mÃºltiplos workers:
    python worker_atacadao.py --workers 2
    
    # Para modo verbose:
    python worker_atacadao.py --verbose
"""

import os
import sys
import logging
import threading
import time
from redis import Redis
from rq import Worker, Queue, Connection
from rq.job import Job
import click
from app.utils.timezone import agora_utc_naive

# Adicionar o diretÃ³rio do projeto ao path
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_redis_connection():
    """Configura conexÃ£o com Redis"""
    redis_url = os.environ.get('REDIS_URL', 'redis://localhost:6379/0')
    logger.info(f"Conectando ao Redis: {redis_url}")
    return Redis.from_url(redis_url)

def worker_startup():
    """Executa ao iniciar o worker"""
    logger.info("="*60)
    logger.info("ğŸš€ WORKER ATACADÃƒO - INICIANDO")
    logger.info(f"ğŸ“… Data/Hora: {agora_utc_naive().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ”§ PID: {os.getpid()}")
    logger.info("="*60)

def worker_shutdown():
    """Executa ao parar o worker"""
    logger.info("="*60)
    logger.info("ğŸ›‘ WORKER ATACADÃƒO - ENCERRANDO")
    logger.info(f"ğŸ“… Data/Hora: {agora_utc_naive().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*60)

# âœ… REMOVIDO Nov/2025: FunÃ§Ã£o run_sendas_scheduler() - automaÃ§Ã£o Playwright descontinuada
# O processamento agora Ã© feito manualmente via exportaÃ§Ã£o de planilha


def job_success_handler(job, connection, result, *args, **kwargs):
    """Handler para jobs bem-sucedidos"""
    logger.info(f"âœ… Job {job.id} concluÃ­do com sucesso")
    logger.info(f"   FunÃ§Ã£o: {job.func_name}")
    logger.info(f"   DuraÃ§Ã£o: {job.ended_at - job.started_at if job.ended_at and job.started_at else 'N/A'}")
    if result:
        logger.info(f"   Resultado: {str(result)[:200]}...")

def job_failure_handler(job, connection, type, value, traceback):
    """Handler para jobs que falharam"""
    logger.error(f"âŒ Job {job.id} falhou!")
    logger.error(f"   FunÃ§Ã£o: {job.func_name}")
    logger.error(f"   Erro: {type.__name__}: {value}")
    logger.error(f"   Args: {job.args}")
    logger.error(f"   Kwargs: {job.kwargs}")

@click.command()
@click.option('--workers', default=1, help='NÃºmero de workers paralelos')
@click.option('--verbose', is_flag=True, help='Modo verbose com mais logs')
@click.option('--burst', is_flag=True, help='Executa jobs pendentes e para')
@click.option('--queues', default='atacadao,high,default', help='Filas a processar (separadas por vÃ­rgula)')
def run_worker(workers, verbose, burst, queues):
    """
    Executa o worker do AtacadÃ£o
    
    Args:
        workers: NÃºmero de workers paralelos
        verbose: Ativa logs detalhados
        burst: Modo burst (executa e para)
        queues: Filas a processar
    """
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Modo verbose ativado")
    
    # Configurar conexÃ£o
    redis_conn = setup_redis_connection()
    
    # Parsear filas
    queue_names = [q.strip() for q in queues.split(',')]
    logger.info(f"ğŸ“‹ Filas monitoradas: {queue_names}")
    
    # Criar objetos Queue
    queues_obj = [Queue(name, connection=redis_conn) for name in queue_names]
    
    # ConfiguraÃ§Ãµes do worker
    worker_config = {
        'name': f'atacadao-worker-{os.getpid()}',
        'connection': redis_conn,
        'queues': queues_obj,
        'log_level': 'DEBUG' if verbose else 'INFO',
        'log_format': '%(asctime)s - %(message)s',
        'date_format': '%Y-%m-%d %H:%M:%S'
    }
    
    # âœ… REMOVIDO Nov/2025: Scheduler Sendas - automaÃ§Ã£o Playwright descontinuada
    # O processamento agora Ã© feito manualmente via exportaÃ§Ã£o de planilha
    logger.info("â„¹ï¸ [Scheduler Sendas] REMOVIDO - usar exportaÃ§Ã£o manual em /portal/sendas/exportacao")

    try:
        with Connection(redis_conn):
            # Startup
            worker_startup()
            
            if workers > 1:
                logger.info(f"ğŸ”„ Iniciando {workers} workers paralelos...")
                
                # Importar multiprocessing
                from multiprocessing import Process
                
                processes = []
                for i in range(workers):
                    p = Process(target=run_single_worker, args=(worker_config, burst))
                    p.start()
                    processes.append(p)
                    logger.info(f"   Worker {i+1} iniciado (PID: {p.pid})")
                
                # Aguardar todos terminarem
                for p in processes:
                    p.join()
            else:
                # Worker Ãºnico
                run_single_worker(worker_config, burst)
            
            # Shutdown
            worker_shutdown()
            
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Interrompido pelo usuÃ¡rio (Ctrl+C)")
        worker_shutdown()
        sys.exit(0)
    except Exception as e:
        logger.error(f"ğŸ’¥ Erro fatal no worker: {e}")
        sys.exit(1)

def run_single_worker(config, burst=False):
    """Executa um worker individual"""
    # Usar PID do processo atual + timestamp + random para garantir nome Ãºnico
    import random
    worker_name = f'atacadao-worker-{os.getpid()}-{int(time.time())}-{random.randint(1000, 9999)}'
    
    worker = Worker(
        name=worker_name,
        queues=config['queues'],
        connection=config['connection'],
        log_job_description=True
    )
    
    # EstatÃ­sticas iniciais
    for queue in config['queues']:
        job_count = len(queue)
        if job_count > 0:
            logger.info(f"ğŸ“¦ Fila '{queue.name}': {job_count} jobs pendentes")
    
    # Executar worker
    logger.info(f"ğŸ‘· Worker '{config['name']}' processando jobs...")
    
    if burst:
        logger.info("ğŸ’¨ Modo BURST ativado - processarÃ¡ jobs existentes e pararÃ¡")
        worker.work(burst=True, logging_level=config['log_level'])
    else:
        logger.info("â™¾ï¸  Modo CONTÃNUO - aguardando novos jobs...")
        worker.work(logging_level=config['log_level'])

def check_status():
    """Verifica status das filas e jobs"""
    redis_conn = setup_redis_connection()
    
    print("\n" + "="*60)
    print("ğŸ“Š STATUS DAS FILAS - ATACADÃƒO")
    print("="*60)
    
    queue_names = ['atacadao', 'high', 'default', 'low']
    
    for queue_name in queue_names:
        queue = Queue(queue_name, connection=redis_conn)
        
        # Jobs pendentes
        pending_count = len(queue)
        
        # Jobs em execuÃ§Ã£o
        started_registry = queue.started_job_registry
        started_count = len(started_registry)
        
        # Jobs concluÃ­dos
        finished_registry = queue.finished_job_registry
        finished_count = len(finished_registry)
        
        # Jobs falhados
        failed_registry = queue.failed_job_registry
        failed_count = len(failed_registry)
        
        print(f"\nğŸ“‹ Fila: {queue_name}")
        print(f"   â³ Pendentes: {pending_count}")
        print(f"   ğŸ”„ Em execuÃ§Ã£o: {started_count}")
        print(f"   âœ… ConcluÃ­dos: {finished_count}")
        print(f"   âŒ Falhados: {failed_count}")
        
        # Mostrar jobs pendentes
        if pending_count > 0 and pending_count <= 5:
            print(f"   ğŸ“¦ Jobs pendentes:")
            for job_id in queue.job_ids[:5]:
                job = Job.fetch(job_id, connection=redis_conn)
                print(f"      - {job.func_name} (ID: {job.id[:8]}...)")
    
    print("\n" + "="*60 + "\n")

if __name__ == '__main__':
    # Se executado com --status, mostra status
    if '--status' in sys.argv:
        check_status()
    else:
        # Executar worker normalmente
        run_worker()