# -*- coding: utf-8 -*-
"""
üß† ANALISADOR NLP AVAN√áADO
Melhora significativa no entendimento usando SpaCy e NLTK
"""

import logging
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass
import re

# Tentativa de importar bibliotecas NLP
try:
    import spacy
    from spacy.lang.pt.stop_words import STOP_WORDS
    NLP_SPACY_AVAILABLE = True
    
    # Tentar carregar modelo portugu√™s
    try:
        nlp = spacy.load("pt_core_news_sm")
    except:
        # Fallback para modelo vazio se n√£o tiver o portugu√™s
        nlp = spacy.blank("pt")
        logging.warning("‚ö†Ô∏è Modelo portugu√™s do spaCy n√£o instalado. Use: python -m spacy download pt_core_news_sm")
except ImportError:
    NLP_SPACY_AVAILABLE = False
    logging.warning("‚ö†Ô∏è SpaCy n√£o instalado. Usando an√°lise b√°sica.")

try:
    from fuzzywuzzy import fuzz, process
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False
    logging.warning("‚ö†Ô∏è FuzzyWuzzy n√£o instalado. Matching exato apenas.")

try:
    import nltk
    from nltk.corpus import stopwords
    # Baixar recursos necess√°rios
    try:
        nltk.download('stopwords', quiet=True)
        nltk.download('punkt', quiet=True)
        NLTK_STOPWORDS = set(stopwords.words('portuguese'))
    except:
        NLTK_STOPWORDS = set()
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    NLTK_STOPWORDS = set()
    logging.warning("‚ö†Ô∏è NLTK n√£o instalado.")

logger = logging.getLogger(__name__)

@dataclass
class AnaliseNLP:
    """Resultado da an√°lise NLP avan√ßada"""
    tokens_limpos: List[str]
    entidades_nomeadas: List[Dict[str, str]]
    palavras_chave: List[str]
    correcoes_sugeridas: Dict[str, str]
    similaridades: Dict[str, float]
    negacoes_detectadas: List[str]
    tempo_verbal: str
    sentimento: str

class NLPEnhancedAnalyzer:
    """Analisador com capacidades NLP avan√ßadas"""
    
    def __init__(self):
        # Dicion√°rio de corre√ß√µes comuns
        self.correcoes_comuns = {
            "assa√≠": "assai",
            "assa√Ø": "assai",
            "asai": "assai",
            "atacadao": "atacad√£o",
            "atacadaum": "atacad√£o",
            "carrefur": "carrefour",
            "carefour": "carrefour",
            "entregass": "entregas",
            "entrega": "entregas",
            "atrazadas": "atrasadas",
            "atrazado": "atrasado",
            "pendente": "pendentes",
            "relatorio": "relat√≥rio",
            "sao paulo": "s√£o paulo",
            "nf": "nota fiscal",
            "cte": "conhecimento de transporte",
            "pdd": "pedido"
        }
        
        # Sin√¥nimos importantes
        self.sinonimos = {
            "atrasado": ["atrasada", "atrasados", "atrasadas", "atraso", "em atraso", "pendente", "vencido"],
            "entrega": ["entregas", "entregue", "entregou", "entregar", "remessa", "envio"],
            "cliente": ["clientes", "empresa", "comprador", "destinat√°rio"],
            "frete": ["fretes", "transporte", "fretamento", "carreto"],
            "pedido": ["pedidos", "ordem", "pdd", "solicita√ß√£o"],
            "nota": ["nf", "nota fiscal", "nfe", "danfe", "documento"],
            "mostrar": ["mostre", "listar", "exibir", "ver", "visualizar", "trazer"],
            "quantidade": ["quantos", "quantas", "total", "n√∫mero", "qtd", "qtde"]
        }
        
        # Palavras de nega√ß√£o
        self.palavras_negacao = {
            "n√£o", "nao", "nunca", "jamais", "nenhum", "nenhuma", 
            "sem", "exceto", "menos", "fora", "al√©m"
        }
        
        logger.info(f"üß† NLP Enhanced Analyzer inicializado")
        logger.info(f"   SpaCy: {'‚úÖ' if NLP_SPACY_AVAILABLE else '‚ùå'}")
        logger.info(f"   Fuzzy: {'‚úÖ' if FUZZY_AVAILABLE else '‚ùå'}")
        logger.info(f"   NLTK: {'‚úÖ' if NLTK_AVAILABLE else '‚ùå'}")
    
    def analisar_com_nlp(self, texto: str) -> AnaliseNLP:
        """An√°lise completa com NLP avan√ßado"""
        
        # Prepara√ß√£o inicial
        texto_lower = texto.lower().strip()
        
        # 1. Corre√ß√µes ortogr√°ficas
        correcoes = self._aplicar_correcoes(texto_lower)
        
        # 2. Tokeniza√ß√£o e limpeza
        if NLP_SPACY_AVAILABLE:
            tokens_limpos = self._tokenizar_spacy(correcoes['texto_corrigido'])
            entidades = self._extrair_entidades_spacy(correcoes['texto_corrigido'])
        else:
            tokens_limpos = self._tokenizar_basico(correcoes['texto_corrigido'])
            entidades = []
        
        # 3. Detec√ß√£o de nega√ß√µes
        negacoes = self._detectar_negacoes(tokens_limpos)
        
        # 4. An√°lise de similaridade
        similaridades = self._calcular_similaridades(correcoes['texto_corrigido'])
        
        # 5. Palavras-chave
        palavras_chave = self._extrair_palavras_chave(tokens_limpos)
        
        # 6. An√°lise de tempo verbal
        tempo_verbal = self._detectar_tempo_verbal(texto_lower)
        
        # 7. Sentimento b√°sico
        sentimento = self._analisar_sentimento_basico(texto_lower)
        
        return AnaliseNLP(
            tokens_limpos=tokens_limpos,
            entidades_nomeadas=entidades,
            palavras_chave=palavras_chave,
            correcoes_sugeridas=correcoes['correcoes_aplicadas'],
            similaridades=similaridades,
            negacoes_detectadas=negacoes,
            tempo_verbal=tempo_verbal,
            sentimento=sentimento
        )
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """Alias para analisar_com_nlp para compatibilidade"""
        resultado = self.analisar_com_nlp(text)
        
        # Converter AnaliseNLP para dict
        return {
            'entities': resultado.entidades_nomeadas,
            'keywords': resultado.palavras_chave,
            'sentiment': resultado.sentimento,
            'complexity': 1.0, # Placeholder, as confianca_analise is not in AnaliseNLP
            'tokens': resultado.tokens_limpos,
            'corrections': resultado.correcoes_sugeridas,
            'similarity_scores': resultado.similaridades,
            'verb_tense': resultado.tempo_verbal,
            'confidence': 1.0
        }
    
    def _aplicar_correcoes(self, texto: str) -> Dict[str, Any]:
        """Aplica corre√ß√µes ortogr√°ficas comuns"""
        texto_corrigido = texto
        correcoes_aplicadas = {}
        
        # Aplicar corre√ß√µes diretas
        for erro, correcao in self.correcoes_comuns.items():
            if erro in texto_corrigido:
                texto_corrigido = texto_corrigido.replace(erro, correcao)
                correcoes_aplicadas[erro] = correcao
        
        # Corre√ß√µes com fuzzy matching se dispon√≠vel
        if FUZZY_AVAILABLE:
            palavras = texto_corrigido.split()
            palavras_corrigidas = []
            
            for palavra in palavras:
                # Verificar se palavra precisa corre√ß√£o
                if len(palavra) > 3:  # Apenas palavras maiores
                    melhores = process.extractOne(
                        palavra, 
                        list(self.correcoes_comuns.values()),
                        scorer=fuzz.ratio
                    )
                    if melhores and melhores[1] > 85:  # 85% de similaridade
                        palavras_corrigidas.append(melhores[0])
                        if palavra != melhores[0]:
                            correcoes_aplicadas[palavra] = melhores[0]
                    else:
                        palavras_corrigidas.append(palavra)
                else:
                    palavras_corrigidas.append(palavra)
            
            texto_corrigido = " ".join(palavras_corrigidas)
        
        return {
            'texto_corrigido': texto_corrigido,
            'correcoes_aplicadas': correcoes_aplicadas
        }
    
    def _tokenizar_spacy(self, texto: str) -> List[str]:
        """Tokeniza√ß√£o usando SpaCy"""
        doc = nlp(texto)
        # Remover stopwords e pontua√ß√£o
        tokens = [
            token.text.lower() for token in doc 
            if not token.is_stop and not token.is_punct and len(token.text) > 1
        ]
        return tokens
    
    def _tokenizar_basico(self, texto: str) -> List[str]:
        """Tokeniza√ß√£o b√°sica sem SpaCy"""
        # Remover pontua√ß√£o
        texto_limpo = re.sub(r'[^\w\s]', ' ', texto)
        # Dividir e filtrar
        palavras = texto_limpo.lower().split()
        # Remover stopwords b√°sicas
        stopwords_basicas = {'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o', 'uma'}
        return [p for p in palavras if p not in stopwords_basicas and len(p) > 1]
    
    def _extrair_entidades_spacy(self, texto: str) -> List[Dict[str, str]]:
        """Extrai entidades nomeadas usando SpaCy"""
        if not NLP_SPACY_AVAILABLE:
            return []
        
        doc = nlp(texto)
        entidades = []
        
        for ent in doc.ents:
            entidades.append({
                'texto': ent.text,
                'tipo': ent.label_,
                'inicio': ent.start_char,
                'fim': ent.end_char
            })
        
        return entidades
    
    def _detectar_negacoes(self, tokens: List[str]) -> List[str]:
        """Detecta palavras de nega√ß√£o no contexto"""
        negacoes_encontradas = []
        
        for i, token in enumerate(tokens):
            if token in self.palavras_negacao:
                # Capturar contexto (palavra anterior e pr√≥xima)
                contexto = []
                if i > 0:
                    contexto.append(tokens[i-1])
                contexto.append(token)
                if i < len(tokens) - 1:
                    contexto.append(tokens[i+1])
                
                negacoes_encontradas.append(" ".join(contexto))
        
        return negacoes_encontradas
    
    def _calcular_similaridades(self, texto: str) -> Dict[str, float]:
        """Calcula similaridade com termos importantes"""
        if not FUZZY_AVAILABLE:
            return {}
        
        similaridades = {}
        termos_importantes = [
            "entregas atrasadas",
            "pedidos pendentes", 
            "relat√≥rio excel",
            "status do sistema",
            "problemas urgentes"
        ]
        
        for termo in termos_importantes:
            score = fuzz.partial_ratio(texto.lower(), termo)
            if score > 60:  # Apenas similaridades relevantes
                similaridades[termo] = score / 100.0
        
        return similaridades
    
    def _extrair_palavras_chave(self, tokens: List[str]) -> List[str]:
        """Extrai palavras-chave mais importantes"""
        # Palavras importantes para o dom√≠nio
        palavras_dominio = {
            'entrega', 'entregas', 'pedido', 'pedidos', 'cliente', 'clientes',
            'frete', 'fretes', 'atraso', 'atrasado', 'atrasada', 'pendente',
            'urgente', 'problema', 'relat√≥rio', 'excel', 'assai', 'atacad√£o',
            'carrefour', 'transportadora', 'nota', 'fiscal', 'cte'
        }
        
        # Filtrar apenas palavras relevantes
        palavras_chave = []
        for token in tokens:
            # Verificar match direto
            if token in palavras_dominio:
                palavras_chave.append(token)
            # Verificar sin√¥nimos
            else:
                for termo, sinonimos in self.sinonimos.items():
                    if token in sinonimos:
                        palavras_chave.append(termo)
                        break
        
        # Remover duplicatas mantendo ordem
        palavras_unicas = []
        for palavra in palavras_chave:
            if palavra not in palavras_unicas:
                palavras_unicas.append(palavra)
        
        return palavras_unicas
    
    def _detectar_tempo_verbal(self, texto: str) -> str:
        """Detecta tempo verbal predominante"""
        # Indicadores simples
        if any(palavra in texto for palavra in ['foi', 'eram', 'estava', 'tinha', 'fez']):
            return "passado"
        elif any(palavra in texto for palavra in ['ser√°', 'vai', 'ir√°', 'quando', 'previs√£o']):
            return "futuro"
        else:
            return "presente"
    
    def _analisar_sentimento_basico(self, texto: str) -> str:
        """An√°lise b√°sica de sentimento"""
        palavras_negativas = {'problema', 'erro', 'atraso', 'urgente', 'cr√≠tico', 'falha', 'ruim'}
        palavras_positivas = {'bom', '√≥timo', 'excelente', 'perfeito', 'sucesso', 'ok'}
        
        score_negativo = sum(1 for palavra in palavras_negativas if palavra in texto)
        score_positivo = sum(1 for palavra in palavras_positivas if palavra in texto)
        
        if score_negativo > score_positivo:
            return "negativo"
        elif score_positivo > score_negativo:
            return "positivo"
        else:
            return "neutro"

# Singleton
_nlp_analyzer = None

def get_nlp_analyzer() -> NLPEnhancedAnalyzer:
    """Retorna inst√¢ncia singleton do analisador NLP"""
    global _nlp_analyzer
    if _nlp_analyzer is None:
        _nlp_analyzer = NLPEnhancedAnalyzer()
    return _nlp_analyzer

# FUN√á√ÉO GET_ √ìRF√É CR√çTICA - ESTAVA FALTANDO!
def get_nlp_enhanced_analyzer() -> NLPEnhancedAnalyzer:
    """Retorna inst√¢ncia do analisador NLP avan√ßado - FUN√á√ÉO √ìRF√É RECUPERADA"""
    return get_nlp_analyzer() 