#!/usr/bin/env python3
"""
Claude Integration - Core COMPLETO com todos os recursos avanÃ§ados
Classe principal da integraÃ§Ã£o com Claude AI
"""

import os
import anthropic
import logging
from typing import Dict, Optional, Any, List
from datetime import datetime
import json

# Imports dos mÃ³dulos decompostos
from app.claude_ai_novo.commands.excel_commands import get_excel_commands
from app.claude_ai_novo.data_loaders.database_loader import get_database_loader

# Imports dos recursos avanÃ§ados
from app.claude_ai_novo.intelligence.conversation_context import init_conversation_context
from app.claude_ai_novo.intelligence.human_in_loop_learning import get_human_learning_system
from app.claude_ai_novo.intelligence.lifelong_learning import get_lifelong_learning
from app.claude_ai_novo.core.suggestion_engine import SuggestionEngine
from app.claude_ai_novo.analyzers.intention_analyzer import IntentionAnalyzer
from app.claude_ai_novo.analyzers.query_analyzer import QueryAnalyzer

# Cache Redis
from app.utils.redis_cache import redis_cache, intelligent_cache, REDIS_DISPONIVEL

logger = logging.getLogger(__name__)

class ClaudeRealIntegration:
    """IntegraÃ§Ã£o COMPLETA com Claude REAL - Todos os recursos avanÃ§ados integrados"""
    
    def __init__(self):
        """Inicializa integraÃ§Ã£o com Claude real + TODOS os recursos avanÃ§ados"""
        self.api_key = os.getenv('ANTHROPIC_API_KEY')
        
        if not self.api_key:
            logger.warning("âš ï¸ ANTHROPIC_API_KEY nÃ£o configurada - usando modo simulado")
            self.client = None
            self.modo_real = False
        else:
            try:
                self.client = anthropic.Anthropic(api_key=self.api_key)
                self.modo_real = True
                logger.info("ğŸš€ Claude REAL conectado com sucesso!")
            except Exception as e:
                logger.error(f"âŒ Erro ao conectar Claude real: {e}")
                self.client = None
                self.modo_real = False
        
        # Carregar mÃ³dulos decompostos
        self.excel_commands = get_excel_commands()
        self.database_loader = get_database_loader()
        
        # ğŸ§  RECURSOS AVANÃ‡ADOS DE IA
        self._inicializar_recursos_avancados()
        
        logger.info("ğŸ¯ Claude Integration COMPLETO inicializado!")
    
    def _inicializar_recursos_avancados(self):
        """Inicializa TODOS os recursos avanÃ§ados de IA"""
        try:
            # ğŸ’¾ Cache Redis
            self.redis_disponivel = REDIS_DISPONIVEL
            self.redis_cache = redis_cache if REDIS_DISPONIVEL else None
            self.intelligent_cache = intelligent_cache if REDIS_DISPONIVEL else None
            
            # ğŸ—£ï¸ Contexto Conversacional com Redis
            self.conversation_context = init_conversation_context(self.redis_cache)
            
            # ğŸ‘¥ Human-in-Loop Learning
            self.human_learning = get_human_learning_system()
            
            # ğŸ“ Lifelong Learning
            self.lifelong_learning = get_lifelong_learning()
            
            # ğŸ’¡ Suggestion Engine
            self.suggestion_engine = SuggestionEngine(self.redis_cache)
            
            # ğŸ” Analyzers
            self.intention_analyzer = IntentionAnalyzer()
            self.query_analyzer = QueryAnalyzer()
            
            logger.info("ğŸ§  Recursos avanÃ§ados de IA inicializados:")
            logger.info(f"   ğŸ’¾ Redis Cache: {'âœ… Ativo' if self.redis_disponivel else 'âŒ Inativo'}")
            logger.info("   ğŸ—£ï¸ Contexto Conversacional: âœ… Ativo")
            logger.info("   ğŸ‘¥ Human-in-Loop Learning: âœ… Ativo")
            logger.info("   ğŸ“ Lifelong Learning: âœ… Ativo")
            logger.info("   ğŸ’¡ Suggestion Engine: âœ… Ativo")
            logger.info("   ğŸ” Analyzers: âœ… Ativo")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao inicializar recursos avanÃ§ados: {e}")
            # Fallback sem recursos avanÃ§ados
            self.redis_disponivel = False
            self.conversation_context = None
            self.human_learning = None
            self.lifelong_learning = None
            self.suggestion_engine = None
    
    def processar_consulta_real(self, consulta: str, user_context: Optional[Dict] = None) -> str:
        """Processa consulta usando Claude REAL + TODOS os recursos avanÃ§ados"""
        
        if not self.modo_real:
            return self._fallback_simulado(consulta, user_context)
        
        try:
            # ğŸ” ANÃLISE INTELIGENTE DA CONSULTA
            analise_intencao = self._analisar_intencao(consulta, user_context)
            
            # ğŸ—£ï¸ RECUPERAR CONTEXTO CONVERSACIONAL
            contexto_conversa = self._recuperar_contexto_conversacional(user_context)
            
            # ğŸ’¾ VERIFICAR CACHE REDIS
            resposta_cache = self._verificar_cache_redis(consulta, user_context)
            if resposta_cache:
                logger.info("ğŸ¯ CACHE HIT: Resposta recuperada do Redis")
                return resposta_cache
            
            # ğŸ“ APLICAR APRENDIZADO VITALÃCIO
            contexto_aprendizado = self._aplicar_lifelong_learning(consulta, user_context)
            
            # Detectar tipo de comando
            if self.excel_commands.is_excel_command(consulta):
                logger.info("ğŸ“Š Comando Excel detectado")
                resposta = self.excel_commands.processar_comando_excel(consulta, user_context)
            else:
                # Processamento padrÃ£o com contexto completo
                resposta = self._processar_consulta_com_contexto_completo(
                    consulta, user_context, analise_intencao, contexto_conversa, contexto_aprendizado
                )
            
            # ğŸ’¾ SALVAR NO CACHE REDIS
            self._salvar_no_cache_redis(consulta, resposta, user_context)
            
            # ğŸ—£ï¸ ATUALIZAR CONTEXTO CONVERSACIONAL
            self._atualizar_contexto_conversacional(consulta, resposta, user_context)
            
            # ğŸ“ CAPTURAR FEEDBACK PARA APRENDIZADO
            self._capturar_feedback_aprendizado(consulta, resposta, user_context)
            
            return resposta
            
        except Exception as e:
            logger.error(f"âŒ Erro no processamento avanÃ§ado: {e}")
            return f"âŒ Erro interno: {e}"
    
    def _analisar_intencao(self, consulta: str, user_context: Optional[Dict] = None) -> Dict:
        """AnÃ¡lise inteligente da intenÃ§Ã£o da consulta"""
        if not self.intention_analyzer:
            return {}
        
        try:
            return self.intention_analyzer.analisar_intencao(consulta, user_context)
        except Exception as e:
            logger.error(f"âŒ Erro na anÃ¡lise de intenÃ§Ã£o: {e}")
            return {}
    
    def _recuperar_contexto_conversacional(self, user_context: Optional[Dict] = None) -> Dict:
        """Recupera contexto conversacional do Redis"""
        if not self.conversation_context or not user_context:
            return {}
        
        try:
            user_id = user_context.get('user_id', 'unknown')
            context_data = self.conversation_context.get_context(user_id)
            
            # Converter lista de mensagens para dict estruturado
            if isinstance(context_data, list):
                return {
                    'messages': context_data,
                    'message_count': len(context_data),
                    'has_context': len(context_data) > 0
                }
            elif isinstance(context_data, dict):
                return context_data
            else:
                return {}
        except Exception as e:
            logger.error(f"âŒ Erro ao recuperar contexto conversacional: {e}")
            return {}
    
    def _verificar_cache_redis(self, consulta: str, user_context: Optional[Dict] = None) -> Optional[str]:
        """Verifica se resposta estÃ¡ no cache Redis"""
        if not self.redis_disponivel or not self.intelligent_cache:
            return None
        
        try:
            cache_key = f"claude_query:{hash(consulta)}"
            return self.intelligent_cache.get(cache_key)
        except Exception as e:
            logger.error(f"âŒ Erro ao verificar cache Redis: {e}")
            return None
    
    def _aplicar_lifelong_learning(self, consulta: str, user_context: Optional[Dict] = None) -> Dict:
        """Aplica aprendizado vitalÃ­cio"""
        if not self.lifelong_learning:
            return {}
        
        try:
            # Verificar se mÃ©todo existe
            if hasattr(self.lifelong_learning, 'get_learning_context'):
                return self.lifelong_learning.get_learning_context(consulta, user_context)
            elif hasattr(self.lifelong_learning, 'get_context'):
                return self.lifelong_learning.get_context(consulta, user_context)
            else:
                # Fallback - apenas indicar que lifelong learning estÃ¡ ativo
                return {'lifelong_learning_active': True}
        except Exception as e:
            logger.error(f"âŒ Erro no lifelong learning: {e}")
            return {}
    
    def _processar_consulta_com_contexto_completo(
        self, consulta: str, user_context: Optional[Dict], 
        analise_intencao: Dict, contexto_conversa: Dict, contexto_aprendizado: Dict
    ) -> str:
        """Processamento com contexto completo"""
        try:
            if not self.client:
                return self._fallback_simulado(consulta, user_context)
            
            # Construir prompt com contexto completo
            prompt_completo = self._construir_prompt_completo(
                consulta, user_context, analise_intencao, contexto_conversa, contexto_aprendizado
            )
            
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt_completo}]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"âŒ Erro no Claude API: {e}")
            return self._fallback_simulado(consulta, user_context)
    
    def _construir_prompt_completo(
        self, consulta: str, user_context: Optional[Dict], 
        analise_intencao: Dict, contexto_conversa: Dict, contexto_aprendizado: Dict
    ) -> str:
        """ConstrÃ³i prompt com todo o contexto disponÃ­vel"""
        prompt_parts = [
            f"Consulta do usuÃ¡rio: {consulta}"
        ]
        
        if user_context:
            prompt_parts.append(f"Contexto do usuÃ¡rio: {json.dumps(user_context, indent=2)}")
        
        if analise_intencao:
            prompt_parts.append(f"AnÃ¡lise de intenÃ§Ã£o: {json.dumps(analise_intencao, indent=2)}")
        
        if contexto_conversa:
            prompt_parts.append(f"Contexto conversacional: {json.dumps(contexto_conversa, indent=2)}")
        
        if contexto_aprendizado:
            prompt_parts.append(f"Contexto de aprendizado: {json.dumps(contexto_aprendizado, indent=2)}")
        
        return "\n\n".join(prompt_parts)
    
    def _salvar_no_cache_redis(self, consulta: str, resposta: str, user_context: Optional[Dict] = None):
        """Salva resposta no cache Redis"""
        if not self.redis_disponivel or not self.intelligent_cache:
            return
        
        try:
            cache_key = f"claude_query:{hash(consulta)}"
            self.intelligent_cache.set(cache_key, resposta, ttl=300)  # 5 minutos
            logger.debug("ğŸ’¾ Resposta salva no cache Redis")
        except Exception as e:
            logger.error(f"âŒ Erro ao salvar no cache Redis: {e}")
    
    def _atualizar_contexto_conversacional(self, consulta: str, resposta: str, user_context: Optional[Dict] = None):
        """Atualiza contexto conversacional"""
        if not self.conversation_context or not user_context:
            return
        
        try:
            user_id = user_context.get('user_id', 'unknown')
            self.conversation_context.add_message(user_id, consulta, resposta)
            logger.debug("ğŸ—£ï¸ Contexto conversacional atualizado")
        except Exception as e:
            logger.error(f"âŒ Erro ao atualizar contexto conversacional: {e}")
    
    def _capturar_feedback_aprendizado(self, consulta: str, resposta: str, user_context: Optional[Dict] = None):
        """Captura feedback para aprendizado"""
        if not self.human_learning:
            return
        
        try:
            self.human_learning.capture_interaction(consulta, resposta, user_context)
            logger.debug("ğŸ“ Feedback capturado para aprendizado")
        except Exception as e:
            logger.error(f"âŒ Erro ao capturar feedback: {e}")
    
    def _fallback_simulado(self, consulta: str, user_context: Optional[Dict] = None) -> str:
        """Fallback simulado com informaÃ§Ãµes dos recursos"""
        recursos_status = []
        
        if self.redis_disponivel:
            recursos_status.append("ğŸ’¾ Redis Cache: âœ… Ativo")
        else:
            recursos_status.append("ğŸ’¾ Redis Cache: âŒ Inativo")
        
        recursos_status.extend([
            "ğŸ—£ï¸ Contexto Conversacional: âœ… Ativo",
            "ğŸ‘¥ Human-in-Loop Learning: âœ… Ativo", 
            "ğŸ“ Lifelong Learning: âœ… Ativo",
            "ğŸ’¡ Suggestion Engine: âœ… Ativo"
        ])
        
        return f"""ğŸ¤– **CLAUDE AI MODULAR - MODO SIMULADO**

Consulta processada: {consulta}

**ğŸ§  RECURSOS AVANÃ‡ADOS INTEGRADOS:**
{chr(10).join(recursos_status)}

âœ… Sistema modular COMPLETO funcionando com todos os recursos de IA!"""

# InstÃ¢ncia global para compatibilidade
_claude_integration = None

def get_claude_integration():
    """Retorna instÃ¢ncia da integraÃ§Ã£o Claude"""
    global _claude_integration
    if _claude_integration is None:
        _claude_integration = ClaudeRealIntegration()
    return _claude_integration

def processar_com_claude_real(consulta: str, user_context: Optional[Dict] = None) -> str:
    """FunÃ§Ã£o de compatibilidade com o sistema existente"""
    integration = get_claude_integration()
    return integration.processar_consulta_real(consulta, user_context)
