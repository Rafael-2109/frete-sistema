"""
üß† SISTEMA DE EMBEDDINGS SEM√ÇNTICOS
Busca avan√ßada usando OpenAI Embeddings para RAG de alta precis√£o
"""

import logging
import numpy as np
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import openai
import pickle
import os
from dataclasses import dataclass, asdict
from sklearn.metrics.pairwise import cosine_similarity
import redis
from app.utils.redis_cache import redis_cache, REDIS_DISPONIVEL

logger = logging.getLogger(__name__)

@dataclass
class DocumentoEmbedding:
    """Documento com embedding sem√¢ntico"""
    id: str
    conteudo: str
    categoria: str
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = None
    timestamp: datetime = None
    relevancia_score: float = 0.0

class SemanticEmbeddingSystem:
    """
    Sistema de embeddings sem√¢nticos para busca avan√ßada de contexto
    """
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        self.embedding_model = "text-embedding-ada-002"
        self.embedding_cache = {}
        self.documentos_indexados = []
        
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
            logger.info("üß† Sistema de Embeddings Sem√¢nticos inicializado com OpenAI")
        else:
            logger.warning("‚ö†Ô∏è OpenAI API Key n√£o encontrada - usando fallback textual")
    
    def gerar_embedding(self, texto: str) -> List[float]:
        """Gera embedding sem√¢ntico para um texto"""
        
        # Cache local primeiro
        texto_hash = hash(texto)
        if texto_hash in self.embedding_cache:
            return self.embedding_cache[texto_hash]
        
        # Cache Redis se dispon√≠vel
        if REDIS_DISPONIVEL:
            cache_key = f"embedding:{texto_hash}"
            cached_embedding = redis_cache.get(cache_key)
            if cached_embedding:
                self.embedding_cache[texto_hash] = cached_embedding
                return cached_embedding
        
        # Gerar novo embedding
        try:
            if self.openai_api_key:
                response = openai.Embedding.create(
                    input=texto,
                    model=self.embedding_model
                )
                embedding = response['data'][0]['embedding']
            else:
                # Fallback: embedding simples baseado em frequ√™ncia de palavras
                embedding = self._embedding_fallback(texto)
            
            # Salvar em cache
            self.embedding_cache[texto_hash] = embedding
            
            if REDIS_DISPONIVEL:
                redis_cache.set(cache_key, embedding, ttl=86400)  # 24h
            
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar embedding: {e}")
            return self._embedding_fallback(texto)
    
    def _embedding_fallback(self, texto: str) -> List[float]:
        """Fallback simples para quando OpenAI n√£o est√° dispon√≠vel"""
        # Embedding simples baseado em TF-IDF manual
        palavras_importantes = [
            'entrega', 'frete', 'cliente', 'prazo', 'atraso', 'agendamento',
            'transportadora', 'embarque', 'pedido', 'faturamento', 'problema',
            'reagendamento', 'protocolo', 'destino', 'origem', 'volume'
        ]
        
        texto_lower = texto.lower()
        embedding = []
        
        for palavra in palavras_importantes:
            # Frequ√™ncia normalizada da palavra
            freq = texto_lower.count(palavra) / len(texto_lower.split())
            embedding.append(freq)
        
        # Padronizar para 1536 dimens√µes (como OpenAI ada-002)
        while len(embedding) < 1536:
            embedding.append(0.0)
        
        return embedding[:1536]
    
    def indexar_dados_sistema(self) -> None:
        """Indexa dados hist√≥ricos do sistema para busca sem√¢ntica"""
        
        logger.info("üîÑ Iniciando indexa√ß√£o sem√¢ntica dos dados...")
        
        try:
            from app import db
            from app.monitoramento.models import EntregaMonitorada
            from app.fretes.models import Frete
            from app.embarques.models import Embarque
            
            # Indexar entregas com problemas/coment√°rios
            entregas_problemas = EntregaMonitorada.query.filter(
                EntregaMonitorada.observacoes_entrega.isnot(None),
                EntregaMonitorada.observacoes_entrega != ''
            ).limit(100).all()
            
            for entrega in entregas_problemas:
                conteudo = f"""
                Cliente: {entrega.nome_cliente}
                Destino: {entrega.destino}
                Status: {entrega.status_finalizacao}
                Observa√ß√µes: {entrega.observacoes_entrega}
                Data Entrega: {entrega.data_entrega_prevista}
                """
                
                doc = DocumentoEmbedding(
                    id=f"entrega_{entrega.id}",
                    conteudo=conteudo.strip(),
                    categoria="entregas_problemas",
                    metadata={
                        'cliente': entrega.nome_cliente,
                        'status': entrega.status_finalizacao,
                        'data_entrega': entrega.data_entrega_prevista.isoformat() if entrega.data_entrega_prevista else None
                    },
                    timestamp=datetime.now()
                )
                
                doc.embedding = self.gerar_embedding(doc.conteudo)
                self.documentos_indexados.append(doc)
            
            # Indexar padr√µes de fretes por transportadora
            fretes_analise = db.session.query(
                Frete.id_transportadora,
                Frete.destino_uf,
                db.func.avg(Frete.valor_cotado).label('valor_medio'),
                db.func.count(Frete.id).label('volume_fretes')
            ).group_by(Frete.id_transportadora, Frete.destino_uf).limit(50).all()
            
            for frete_pattern in fretes_analise:
                conteudo = f"""
                Transportadora ID: {frete_pattern.id_transportadora}
                Destino UF: {frete_pattern.destino_uf}
                Valor M√©dio: R$ {frete_pattern.valor_medio:.2f}
                Volume: {frete_pattern.volume_fretes} fretes
                Padr√£o: frete m√©dio para {frete_pattern.destino_uf}
                """
                
                doc = DocumentoEmbedding(
                    id=f"frete_pattern_{frete_pattern.id_transportadora}_{frete_pattern.destino_uf}",
                    conteudo=conteudo.strip(),
                    categoria="padroes_frete",
                    metadata={
                        'transportadora_id': frete_pattern.id_transportadora,
                        'uf': frete_pattern.destino_uf,
                        'valor_medio': float(frete_pattern.valor_medio),
                        'volume': frete_pattern.volume_fretes
                    },
                    timestamp=datetime.now()
                )
                
                doc.embedding = self.gerar_embedding(doc.conteudo)
                self.documentos_indexados.append(doc)
            
            logger.info(f"‚úÖ Indexa√ß√£o conclu√≠da: {len(self.documentos_indexados)} documentos")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na indexa√ß√£o: {e}")
    
    def buscar_contexto_semantico(self, consulta: str, top_k: int = 5, categoria_filtro: str = None) -> List[DocumentoEmbedding]:
        """Busca contexto relevante usando similaridade sem√¢ntica"""
        
        if not self.documentos_indexados:
            self.indexar_dados_sistema()
        
        # Gerar embedding da consulta
        consulta_embedding = self.gerar_embedding(consulta)
        
        # Calcular similaridades
        documentos_relevantes = []
        
        for doc in self.documentos_indexados:
            # Filtrar por categoria se especificado
            if categoria_filtro and doc.categoria != categoria_filtro:
                continue
            
            # Calcular similaridade do cosseno
            if doc.embedding:
                similarity = cosine_similarity(
                    [consulta_embedding],
                    [doc.embedding]
                )[0][0]
                
                doc.relevancia_score = similarity
                
                # Threshold m√≠nimo de relev√¢ncia
                if similarity > 0.7:  # Apenas muito relevantes
                    documentos_relevantes.append(doc)
        
        # Ordenar por relev√¢ncia e retornar top_k
        documentos_relevantes.sort(key=lambda x: x.relevancia_score, reverse=True)
        
        logger.info(f"üîç Busca sem√¢ntica: {len(documentos_relevantes)} docs relevantes (threshold >0.7)")
        
        return documentos_relevantes[:top_k]
    
    def analisar_intencao_consulta(self, consulta: str) -> Dict[str, Any]:
        """Analisa a inten√ß√£o da consulta usando embeddings sem√¢nticos"""
        
        # Padr√µes de inten√ß√£o com embeddings pr√©-calculados
        intencoes_padrao = {
            "analise_performance": "an√°lise de performance entregas pontualidade prazo kpi indicadores",
            "resolucao_problema": "problema erro falha resolver ajudar suporte troubleshooting",
            "previsao_demanda": "previs√£o demanda volume futuro planejamento capacidade forecasting",
            "comparacao_clientes": "comparar clientes performance diferen√ßa ranking",
            "status_operacional": "status situa√ß√£o como est√° andamento opera√ß√£o atual",
            "custos_financeiro": "custo valor pre√ßo financeiro or√ßamento investimento",
            "otimizacao_rotas": "rota otimizar melhorar efici√™ncia reduzir tempo"
        }
        
        consulta_embedding = self.gerar_embedding(consulta)
        
        melhor_intencao = "geral"
        melhor_score = 0.0
        
        for intencao, padrao in intencoes_padrao.items():
            padrao_embedding = self.gerar_embedding(padrao)
            
            similarity = cosine_similarity(
                [consulta_embedding],
                [padrao_embedding]
            )[0][0]
            
            if similarity > melhor_score:
                melhor_score = similarity
                melhor_intencao = intencao
        
        return {
            "intencao_detectada": melhor_intencao,
            "confianca": melhor_score,
            "sugere_template": melhor_intencao if melhor_score > 0.75 else "geral"
        }
    
    def construir_prompt_semantico(self, consulta: str, dados_sistema: Dict[str, Any]) -> str:
        """Constr√≥i prompt enriquecido com contexto sem√¢ntico"""
        
        # 1. Analisar inten√ß√£o
        analise_intencao = self.analisar_intencao_consulta(consulta)
        
        # 2. Buscar contexto sem√¢ntico relevante
        contexto_semantico = self.buscar_contexto_semantico(consulta, top_k=3)
        
        # 3. Construir contexto enriquecido
        contexto_prompt = f"""
üß† **AN√ÅLISE SEM√ÇNTICA DA CONSULTA**
- Inten√ß√£o detectada: {analise_intencao['intencao_detectada']}
- Confian√ßa: {analise_intencao['confianca']:.2%}
- Template sugerido: {analise_intencao['sugere_template']}

üìä **DADOS DO SISTEMA**
{json.dumps(dados_sistema, indent=2, ensure_ascii=False)}

üîç **CONTEXTO SEM√ÇNTICO RELEVANTE**
"""
        
        if contexto_semantico:
            for i, doc in enumerate(contexto_semantico, 1):
                contexto_prompt += f"""
**Documento {i}** (Relev√¢ncia: {doc.relevancia_score:.2%})
Categoria: {doc.categoria}
Conte√∫do: {doc.conteudo[:300]}...
Metadata: {json.dumps(doc.metadata, ensure_ascii=False)}
---"""
        else:
            contexto_prompt += "\nNenhum contexto hist√≥rico espec√≠fico encontrado."
        
        contexto_prompt += f"""

üí° **INSTRU√á√ïES ESPEC√çFICAS PARA A INTEN√á√ÉO '{analise_intencao['intencao_detectada'].upper()}'**
"""
        
        # Instru√ß√µes espec√≠ficas baseadas na inten√ß√£o
        instrucoes_intencao = {
            "analise_performance": """
- Foque em KPIs e m√©tricas de performance
- Compare com benchmarks do setor
- Identifique tend√™ncias e padr√µes
- Sugira melhorias espec√≠ficas""",
            
            "resolucao_problema": """
- Identifique a causa raiz do problema
- Forne√ßa solu√ß√µes imediatas e de longo prazo
- Use o contexto hist√≥rico para solu√ß√µes similares
- Inclua steps acion√°veis""",
            
            "previsao_demanda": """
- Analise padr√µes hist√≥ricos e sazonais
- Considere fatores externos
- Forne√ßa previs√µes quantitativas
- Inclua cen√°rios (otimista/realista/pessimista)""",
            
            "comparacao_clientes": """
- Compare m√©tricas entre clientes
- Identifique outliers e padr√µes
- Rankear por performance
- Destacar oportunidades de melhoria""",
            
            "status_operacional": """
- Forne√ßa vis√£o atual clara e objetiva
- Destaque pontos de aten√ß√£o
- Use indicadores visuais
- Inclua pr√≥ximos passos""",
            
            "custos_financeiro": """
- Analise custos e ROI
- Identifique oportunidades de economia
- Compare cen√°rios
- Forne√ßa recomenda√ß√µes financeiras""",
            
            "otimizacao_rotas": """
- Analise efici√™ncia atual
- Identifique gargalos
- Sugira otimiza√ß√µes pr√°ticas
- Calcule impacto potencial"""
        }
        
        contexto_prompt += instrucoes_intencao.get(
            analise_intencao['intencao_detectada'],
            "- Forne√ßa an√°lise abrangente e insights acion√°veis"
        )
        
        return contexto_prompt
    
    def salvar_cache_embeddings(self, filepath: str = "cache_embeddings.pkl"):
        """Salva cache de embeddings em arquivo"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump({
                    'cache': self.embedding_cache,
                    'documentos': self.documentos_indexados,
                    'timestamp': datetime.now()
                }, f)
            logger.info(f"üíæ Cache de embeddings salvo em {filepath}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar cache: {e}")
    
    def carregar_cache_embeddings(self, filepath: str = "cache_embeddings.pkl"):
        """Carrega cache de embeddings de arquivo"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    cache_data = pickle.load(f)
                    
                self.embedding_cache = cache_data.get('cache', {})
                self.documentos_indexados = cache_data.get('documentos', [])
                
                logger.info(f"üìÇ Cache de embeddings carregado: {len(self.documentos_indexados)} docs")
                return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar cache: {e}")
        
        return False

class AdvancedPromptChaining:
    """
    Sistema de encadeamento de prompts para consultas complexas
    """
    
    def __init__(self, embedding_system: SemanticEmbeddingSystem):
        self.embedding_system = embedding_system
    
    def processar_consulta_multi_step(self, consulta: str, dados_sistema: Dict[str, Any]) -> str:
        """
        Processa consultas complexas em m√∫ltiplas etapas
        """
        
        # Step 1: Analisar complexidade da consulta
        complexidade = self._analisar_complexidade(consulta)
        
        if complexidade['nivel'] == 'simples':
            # Processamento direto
            return self.embedding_system.construir_prompt_semantico(consulta, dados_sistema)
        
        elif complexidade['nivel'] == 'multipla':
            # Quebrar em sub-consultas
            sub_consultas = self._quebrar_consulta(consulta)
            
            prompt_encadeado = "üîó **CONSULTA MULTI-STEP DETECTADA**\\n\\n"
            
            for i, sub_consulta in enumerate(sub_consultas, 1):
                prompt_encadeado += f"""
**STEP {i}**: {sub_consulta}
{self.embedding_system.construir_prompt_semantico(sub_consulta, dados_sistema)}
---
"""
            
            prompt_encadeado += """
üí° **INSTRU√á√ïES FINAIS**:
1. Processe cada STEP sequencialmente
2. Use resultados anteriores para steps seguintes
3. Forne√ßa resposta integrada final
4. Destaque conex√µes entre os steps
"""
            
            return prompt_encadeado
        
        else:  # complexidade avan√ßada
            return self._processar_consulta_avancada(consulta, dados_sistema)
    
    def _analisar_complexidade(self, consulta: str) -> Dict[str, Any]:
        """Analisa complexidade da consulta"""
        
        indicadores_complexos = [
            'comparar', 'e tamb√©m', 'al√©m disso', 'ao mesmo tempo',
            'correla√ß√£o', 'impacto', 'consequ√™ncia', 'se ent√£o'
        ]
        
        indicadores_multiplos = [
            'primeiro', 'segundo', 'depois', 'em seguida',
            'por √∫ltimo', 'antes de', 'ap√≥s', 'simultaneamente'
        ]
        
        consulta_lower = consulta.lower()
        
        if any(ind in consulta_lower for ind in indicadores_complexos):
            return {'nivel': 'avancada', 'razao': 'consulta_complexa'}
        elif any(ind in consulta_lower for ind in indicadores_multiplos):
            return {'nivel': 'multipla', 'razao': 'multi_step'}
        else:
            return {'nivel': 'simples', 'razao': 'consulta_direta'}
    
    def _quebrar_consulta(self, consulta: str) -> List[str]:
        """Quebra consulta complexa em sub-consultas"""
        
        # Padr√µes para quebrar consulta
        separadores = [
            r'e tamb√©m', r'al√©m disso', r'ao mesmo tempo',
            r'depois', r'em seguida', r'por √∫ltimo'
        ]
        
        import re
        
        sub_consultas = [consulta]
        
        for separador in separadores:
            novas_consultas = []
            for sub in sub_consultas:
                partes = re.split(separador, sub, flags=re.IGNORECASE)
                novas_consultas.extend([p.strip() for p in partes if p.strip()])
            sub_consultas = novas_consultas
        
        return sub_consultas[:5]  # M√°ximo 5 sub-consultas
    
    def _processar_consulta_avancada(self, consulta: str, dados_sistema: Dict[str, Any]) -> str:
        """Processa consultas muito complexas com an√°lise avan√ßada"""
        
        return f"""
üß† **CONSULTA AVAN√áADA DETECTADA**

{self.embedding_system.construir_prompt_semantico(consulta, dados_sistema)}

üî¨ **AN√ÅLISE AVAN√áADA REQUERIDA**:
1. Identifique todas as vari√°veis mencionadas
2. Analise correla√ß√µes e depend√™ncias
3. Considere fatores externos e contexto hist√≥rico
4. Forne√ßa an√°lise multi-dimensional
5. Inclua cen√°rios alternativos
6. Sugira investiga√ß√µes adicionais se necess√°rio

‚ö° **PROCESSAMENTO**: Use Claude 4 Sonnet em modo de racioc√≠nio extendido
"""

# Inst√¢ncias globais
semantic_system = SemanticEmbeddingSystem()
prompt_chaining = AdvancedPromptChaining(semantic_system)

def processar_com_embeddings_semanticos(consulta: str, dados_sistema: Dict[str, Any]) -> str:
    """
    Fun√ß√£o principal para processar consultas com embeddings sem√¢nticos
    """
    return prompt_chaining.processar_consulta_multi_step(consulta, dados_sistema) 